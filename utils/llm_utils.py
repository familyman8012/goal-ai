from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import streamlit as st
from dotenv import load_dotenv
import os
from langchain.callbacks.base import BaseCallbackHandler


def get_api_key(key_name: str) -> str:
    api_key = None
    if hasattr(st, "secrets"):  # Streamlit Cloud í™˜ê²½
        try:
            api_key = st.secrets["api_keys"][key_name]
        except KeyError:
            raise ValueError(
                f"Streamlit secretsì—ì„œ {key_name}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
    else:  # ë¡œì»¬ í™˜ê²½
        load_dotenv()
        api_key = os.getenv(key_name)

    if not api_key:
        raise ValueError(f"{key_name} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    return api_key


class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text
        self.placeholder = self.container.empty()

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.placeholder.markdown(self.text + "â–Œ")

    def on_llm_end(self, *args, **kwargs) -> None:
        self.placeholder.markdown(self.text)


class ChatMemory:
    def __init__(self, session_id: str, max_pairs: int = 3):
        self.history_key = f"chat_history_{session_id}"
        self.max_pairs = max_pairs
        self.buffer_key = f"chat_buffer_{session_id}"
        self.display_key = f"chat_display_{session_id}"

        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if self.history_key not in st.session_state:
            st.session_state[self.history_key] = []
        if self.buffer_key not in st.session_state:
            st.session_state[self.buffer_key] = []
        if self.display_key not in st.session_state:
            st.session_state[self.display_key] = []

    def add_message(self, role: str, content: str):
        # ë©”ì‹œì§€ ê°ì²´ ìƒì„±
        if role == "user":
            message = HumanMessage(content=content)
        elif role == "assistant":
            message = AIMessage(content=content)
        else:
            message = SystemMessage(content=content)

        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” historyì˜ ì²« ë²ˆì§¸ ìœ„ì¹˜ì—ë§Œ ì €ì¥
        if isinstance(message, SystemMessage):
            if not st.session_state[self.history_key] or not isinstance(
                st.session_state[self.history_key][0], SystemMessage
            ):
                st.session_state[self.history_key].insert(0, message)
        else:
            # ì¼ë°˜ ë©”ì‹œì§€ëŠ” ë²„í¼ì™€ displayì— ì¶”ê°€
            st.session_state[self.buffer_key].append(message)
            st.session_state[self.display_key].append(message)

            # ë²„í¼ì˜ ë©”ì‹œì§€ ìŒ ìˆ˜ ê³„ì‚° (user-ai ìŒ)
            message_pairs = len(st.session_state[self.buffer_key]) // 2

            # ëŒ€í™” ìŒì´ max_pairsë¥¼ ì´ˆê³¼í•˜ë©´ ìš”ì•½
            if message_pairs >= self.max_pairs:
                self._move_to_history()

    def get_messages(self):
        # LLMìš© ë©”ì‹œì§€ (ìš”ì•½ í¬í•¨)
        all_messages = []

        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        for msg in st.session_state[self.history_key]:
            if isinstance(msg, SystemMessage):
                all_messages.append(msg)
                break

        # ìš”ì•½ëœ ë©”ì‹œì§€ ì¶”ê°€
        for msg in st.session_state[self.history_key]:
            if (
                isinstance(msg, AIMessage)
                and "[ì´ì „ ëŒ€í™” ìš”ì•½]" in msg.content
            ):
                all_messages.append(msg)

        # í˜„ì¬ ë²„í¼ì˜ ë©”ì‹œì§€ë“¤ ì¶”ê°€
        all_messages.extend(st.session_state[self.buffer_key])

        return all_messages

    def get_display_messages(self):
        # UI í‘œì‹œ ë©”ì‹œì§€ (ì „ì²´ ëŒ€í™” ë‚´ì—­)
        return st.session_state[self.display_key]

    def _move_to_history(self):
        # ë²„í¼ì˜ ëª¨ë“  ë©”ì‹œì§€ë¥¼ í•œë²ˆì— ìš”ì•½
        messages_to_summarize = st.session_state[self.buffer_key]
        # ë²„í¼ ë¹„ìš°ê¸°
        st.session_state[self.buffer_key] = []

        if messages_to_summarize:
            summary = self._create_summary(messages_to_summarize)
            # ìš”ì•½ì„ AIMessageë¡œ ë³€í™˜
            summary_message = AIMessage(content=f"[ì´ì „ ëŒ€í™” ìš”ì•½] {summary}")

            # historyì— ì¶”ê°€ (ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ê·¸ ë‹¤ìŒì— ìš”ì•½ ì¶”ê°€)
            new_history = []
            system_messages_added = False

            # ë¨¼ì € ì‹œìŠ¤í…œ ë©”ì‹œì§€ë“¤ì„ ì¶”ê°€
            for msg in st.session_state[self.history_key]:
                if isinstance(msg, SystemMessage):
                    new_history.append(msg)
                else:
                    if not system_messages_added:
                        new_history.append(summary_message)
                        system_messages_added = True
                    new_history.append(msg)

            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ì—†ì—ˆë˜ ê²½ìš° ë§ˆì§€ë§‰ì— ì¶”ê°€
            if not system_messages_added:
                new_history.append(summary_message)

            st.session_state[self.history_key] = new_history

    def _create_summary(self, messages):
        try:
            # ëŒ€í™” ë‚´ìš©ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ êµ¬ì„±
            conversation_parts = []
            for msg in messages:
                if isinstance(msg, HumanMessage):
                    conversation_parts.append({"role": "user", "content": msg.content})
                elif isinstance(msg, AIMessage):
                    conversation_parts.append({"role": "assistant", "content": msg.content})

            # ìš”ì•½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            summary_prompt = """
            ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ 150ì ì´ë‚´ë¡œ í•µì‹¬ë§Œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:

            {conversation}
            
            ìš”ì•½ í˜•ì‹:
            í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™”ë¥¼ ì‚¬ìš©ìì™€ ai ê°€ ë¬´ìŠ¨ ëŒ€í™”ë¥¼ í–ˆì—ˆëŠ”ì§€ í•µì‹¬ë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
            
            ì£¼ì˜ì‚¬í•­:
            1. 150ìë¥¼ ë„˜ì§€ ì•Šë„ë¡ í•  ê²ƒ
            2. ì¤‘ìš” í‚¤ì›Œë“œëŠ” ë°˜ë“œì‹œ í¬í•¨í•  ê²ƒ
            3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë‚ ì§œëŠ” ìœ ì§€í•  ê²ƒ
            """

            # ëŒ€í™” ë‚´ìš©ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
            conversation_text = "\n".join([
                f"{'ì‚¬ìš©ì' if msg['role'] == 'user' else 'AI'}: {msg['content']}"
                for msg in conversation_parts
            ])

            # ìš”ì•½ì„ ìœ„í•´ Gemini-Flash ëª¨ë¸ ì‚¬ìš©
            llm = LLMFactory.create_llm("gemini-1.5-flash-latest")
            response = llm.invoke([
                HumanMessage(content=summary_prompt.format(conversation=conversation_text))
            ])
            
            # ì„ì‹œ UIë¡œ ìš”ì•½ ê³¼ì • í‘œì‹œ
            with st.expander("ğŸ” ìš”ì•½ ë””ë²„ê·¸"):
                st.write("### í˜„ì¬ ìš”ì•½ ê³¼ì •")
                st.write("ì›ë³¸ ëŒ€í™”:")
                st.write(conversation_text)
                st.write("---")
                st.write("ìƒˆë¡œìš´ ìš”ì•½ ê²°ê³¼ (Gemini-Flash):")
                st.write(response.content)
                
                st.write("\n### ì „ì²´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸")
                st.write("ì‹œìŠ¤í…œ ë©”ì‹œì§€:")
                for msg in st.session_state[self.history_key]:
                    if isinstance(msg, SystemMessage):
                        st.write(msg.content)
                
                st.write("\nìš”ì•½ëœ ì´ì „ ëŒ€í™”:")
                for msg in st.session_state[self.history_key]:
                    if isinstance(msg, AIMessage) and "[ì´ì „ ëŒ€í™” ìš”ì•½]" in msg.content:
                        st.write(msg.content)
                
                st.write("\ní˜„ì¬ ë²„í¼ì˜ ëŒ€í™”:")
                for msg in st.session_state[self.buffer_key]:
                    role = "ì‚¬ìš©ì" if isinstance(msg, HumanMessage) else "AI"
                    st.write(f"{role}: {msg.content}")
       
                
            # ì‘ë‹µ ê²€ì¦ ê°•í™”
            summary = response.content
            if not summary or len(summary) < 10:
                raise ValueError("ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
            
            if len(summary) > 400:  # 200ì ì œí•œì˜ 2ë°°ê¹Œì§€ í—ˆìš©
                raise ValueError("ì‘ë‹µì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤")
            

            
            # í•µ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ ê²€ì¦ (ì˜µì…˜)
            # important_keywords = self._extract_keywords(messages)
            # if not any(keyword in summary for keyword in important_keywords):
            #     raise ValueError("ì¤‘ìš” í‚¤ì›Œë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤")
            
            return summary

        except Exception as e:
            st.warning(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return self._create_fallback_summary(messages)

    def _create_fallback_summary(self, messages):
        fallback_summary = "ëŒ€í™” ìš”ì•½:\n"
        for msg in messages[-2:]:  # ë§ˆì§€ë§‰ 2ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨
            role = "ì‚¬ìš©ì" if isinstance(msg, HumanMessage) else "AI"
            content = msg.content
            if len(content) > 50:
                content = content[:47] + "..."
            fallback_summary += f"- {role}: {content}\n"
        
        return fallback_summary


class LLMFactory:
    @staticmethod
    def create_llm(model_name: str):
        try:
            if model_name.startswith("gpt"):
                model_version = model_name.split("-", 1)[1]
                return ChatOpenAI(
                    api_key=get_api_key("OPENAI_API_KEY"),
                    model_name=f"gpt-{model_version}",
                    temperature=0.7,
                    streaming=True,
                )
            elif model_name.startswith("claude"):
                model_version = model_name.split("-", 1)[1]
                return ChatAnthropic(
                    anthropic_api_key=get_api_key("ANTHROPIC_API_KEY"),
                    model_name=f"claude-{model_version}",
                    temperature=0.7,
                    streaming=True,
                )
            elif model_name.startswith("gemini"):
                api_key = get_api_key("GOOGLE_API_KEY")
                if not api_key:
                    raise ValueError("Google API ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                model_version = model_name.split("-", 1)[1]
                llm = ChatGoogleGenerativeAI(
                    google_api_key=api_key,
                    model=f"gemini-{model_version}",  # ëª¨ë¸ëª… ë™ì  ì„¤ì •
                    temperature=0.7,
                    streaming=False,  # GeminiëŠ” ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì›
                )

                return llm

            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model_name}")

        except Exception as e:
            error_msg = f"LLM ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            st.error(error_msg)
            raise Exception(error_msg)

    @staticmethod
    def get_response(
        model_name: str,
        system_prompt: str,
        user_input: str,
        session_id: str,
        stream_handler: StreamHandler = None,
    ) -> str:
        try:
            llm = LLMFactory.create_llm(model_name)
            memory = ChatMemory(session_id)
            messages = memory.get_messages()

            # ì œë¯¸ë‹ˆ ëª¨ë¸ì„ ìœ„í•œ íŠ¹ë³„ ì²˜ë¦¬
            if model_name.startswith("gemini"):
                try:
                    # ê¸°ì¡´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ í™œìš©í•˜ë„ë¡ ìˆ˜ì •
                    if not messages or not isinstance(messages[0], SystemMessage):
                        memory.add_message("system", system_prompt)
                        messages = memory.get_messages()

                    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
                    memory.add_message("user", user_input)
                    messages = memory.get_messages()

                    # ë©”ì‹œì§€ í˜•ì‹ì„ Geminiìš©ìœ¼ë¡œ ë³€í™˜
                    gemini_messages = []
                    for msg in messages:
                        if isinstance(msg, SystemMessage):
                            gemini_messages.append(
                                HumanMessage(content=f"ì‹œìŠ¤í…œ ì„¤ì •: {msg.content}")
                            )
                        elif isinstance(msg, HumanMessage):
                            gemini_messages.append(msg)
                        elif isinstance(msg, AIMessage):
                            gemini_messages.append(msg)

                    # ì‘ë‹µ ìƒì„± ì‹œë„
                    response = llm.invoke(gemini_messages)

                    # ì‘ë‹µ í‘œì‹œ (ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‹  ì§ì ‘ í‘œì‹œ)
                    if stream_handler:
                        stream_handler.placeholder.empty()
                        stream_handler.container.markdown(response.content)

                    # AI ì‘ë‹µ ì €ì¥
                    memory.add_message("assistant", response.content)
                    return response.content

                except Exception as gemini_error:
                    st.error(f"Gemini ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(gemini_error)}")
                    raise gemini_error

            else:
                # ê¸°ì¡´ ë¡œì§ (ë‹¤ë¥¸ ëª¨ë¸ë“¤)
                if not messages or not isinstance(messages[0], SystemMessage):
                    memory.add_message("system", system_prompt)
                    messages = memory.get_messages()
                memory.add_message("user", user_input)
                messages = memory.get_messages()

                response = llm.invoke(
                    messages,
                    config={"callbacks": ([stream_handler] if stream_handler else None)}
                )

                # AI ì‘ë‹µ ì €ì¥
                memory.add_message("assistant", response.content)
                return response.content

        except Exception as e:
            error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            if stream_handler:
                stream_handler.placeholder.error(error_msg)
            st.error(f"ìƒì„¸ ì˜¤ë¥˜: {str(e)}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. (ì—ëŸ¬: {str(e)})"
